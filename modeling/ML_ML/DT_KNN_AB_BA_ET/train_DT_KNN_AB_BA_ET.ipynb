{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,classification_report\n",
    "import pickle \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of base-models\n",
    "def get_models():\n",
    "    models = list()\n",
    "    models.append(DecisionTreeClassifier())\n",
    "    models.append(KNeighborsClassifier())\n",
    "    models.append(AdaBoostClassifier())\n",
    "    models.append(BaggingClassifier(n_estimators=10))\n",
    "    models.append(ExtraTreesClassifier(n_estimators=10))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect out of fold predictions form k-fold cross validation\n",
    "def get_out_of_fold_predictions(X, y, models):\n",
    "    meta_X, meta_y = list(), list()\n",
    "    # define split of data\n",
    "    kfold = KFold(n_splits=2, shuffle=True)\n",
    "    # enumerate splits\n",
    "    for train_ix, test_ix in kfold.split(X):\n",
    "        fold_yhats = list()\n",
    "        # get data\n",
    "        train_X, test_X = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        train_y, test_y = y.iloc[train_ix], y.iloc[test_ix]\n",
    "        meta_y.extend(test_y)\n",
    "        # fit and make predictions with each sub-model\n",
    "        for model in models:\n",
    "            model.fit(train_X, train_y)\n",
    "            yhat = model.predict_proba(test_X)\n",
    "            # store columns\n",
    "            fold_yhats.append(yhat)\n",
    "        # store fold yhats as columns\n",
    "        meta_X.append(hstack(fold_yhats))\n",
    "    return vstack(meta_X), asarray(meta_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit all base models on the training dataset\n",
    "def fit_base_models(X, y, models):\n",
    "    for model in models:\n",
    "        model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a meta model\n",
    "def fit_meta_model(X, y):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a list of models on a dataset\n",
    "def evaluate_models(X, ytest, models):\n",
    "    for model in models:\n",
    "        y_pred = model.predict(X)\n",
    "        print(model.__class__.__name__)\n",
    "        print(classification_report(ytest,y_pred))\n",
    "        print('precision_score : '+str(precision_score(ytest, y_pred, average='weighted')))\n",
    "        print('accuracy_score : '+str(accuracy_score(ytest, y_pred)))\n",
    "        print('recall_score : '+str(recall_score(ytest, y_pred, average='weighted')))\n",
    "        print('f1_score : '+str(f1_score(ytest, y_pred, average='weighted')))\n",
    "        print('roc_auc_score : '+str(roc_auc_score(ytest,y_pred))) # TruePositive,TrueNegative\n",
    "        tn, fp, fn, tp = confusion_matrix(ytest, y_pred).ravel()\n",
    "        print('True_positive : '+str(tp)+', False_positive : '+str(fp)+', True_negative : '+str(tn)+', False_negative : '+str(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(models, combiner_model):\n",
    "    models_names=[]\n",
    "    for model in models:\n",
    "        with open(\"./models/\"+model.__class__.__name__+\".pickle\",\"wb\") as f:\n",
    "            pickle.dump(model,f)\n",
    "        models_names.append(model.__class__.__name__)\n",
    "    with open(\"./models/models_names.pickle\",\"wb\") as f:\n",
    "            pickle.dump(models_names,f)\n",
    "    with open(\"./models/combiner_model.pickle\",\"wb\") as f:\n",
    "            pickle.dump(combiner_model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions with stacked model\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "    meta_X = list()\n",
    "    for model in models:\n",
    "        yhat = model.predict_proba(X)\n",
    "        meta_X.append(yhat)\n",
    "    meta_X = hstack(meta_X)\n",
    "    # predict\n",
    "    return meta_model.predict(meta_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_df(df):\n",
    "    dic={}\n",
    "    for c in df.columns :\n",
    "        if (df[c].dtype ==\"object\"):\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(df[c])\n",
    "            df[c]=encoder.transform(df[c])\n",
    "            dic[c]=encoder\n",
    "    with open(\"./models/LabelEncoders_dic.pickle\",\"wb\") as f:\n",
    "        pickle.dump(dic,f)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def shuffle_dataframe(df):\n",
    "    df = shuffle(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliser_all_columns(df):\n",
    "    diction={}\n",
    "    \n",
    "    for c in df.columns :\n",
    "        scaler=MinMaxScaler(feature_range=(0,1)).fit(df[c].values.reshape(-1,1))\n",
    "        diction[c]=scaler        \n",
    "        df[c]=scaler.transform(df[c].values.reshape(-1,1))\n",
    "    with open(\"./models/MinMaxScalers_dic.pickle\",\"wb\") as f:\n",
    "        pickle.dump(diction,f)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278060"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.read_csv(\"../../../data/full_data_small_datased.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=transformer_df(df)\n",
    "df=normaliser_all_columns(df)\n",
    "df=shuffle_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the inputs and outputs\n",
    "X, X_val, y, y_val=train_test_split(df.drop(labels=[\"Class\"],axis=1),df[\"Class\"],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (222448, 115) (222448,) Test (55612, 115) (55612,)\n",
      "Meta  (222448, 10) (222448,)\n"
     ]
    }
   ],
   "source": [
    "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
    "# get models\n",
    "models = get_models()\n",
    "# get out of fold predictions\n",
    "start_time = time.time() \n",
    "meta_X, meta_y = get_out_of_fold_predictions(X, y, models)\n",
    "print('Meta ', meta_X.shape, meta_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit base models\n",
    "fit_base_models(X, y, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 328.96815371513367 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# fit the meta model\n",
    "meta_model = fit_meta_model(meta_X, meta_y)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      8073\n",
      "         1.0       1.00      1.00      1.00     47539\n",
      "\n",
      "    accuracy                           1.00     55612\n",
      "   macro avg       1.00      1.00      1.00     55612\n",
      "weighted avg       1.00      1.00      1.00     55612\n",
      "\n",
      "precision_score : 1.0\n",
      "accuracy_score : 1.0\n",
      "recall_score : 1.0\n",
      "f1_score : 1.0\n",
      "roc_auc_score : 1.0\n",
      "True_positive : 47539, False_positive : 0, True_negative : 8073, False_negative : 0\n",
      "KNeighborsClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      8073\n",
      "         1.0       1.00      1.00      1.00     47539\n",
      "\n",
      "    accuracy                           1.00     55612\n",
      "   macro avg       1.00      1.00      1.00     55612\n",
      "weighted avg       1.00      1.00      1.00     55612\n",
      "\n",
      "precision_score : 1.0\n",
      "accuracy_score : 1.0\n",
      "recall_score : 1.0\n",
      "f1_score : 1.0\n",
      "roc_auc_score : 1.0\n",
      "True_positive : 47539, False_positive : 0, True_negative : 8073, False_negative : 0\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      8073\n",
      "         1.0       1.00      1.00      1.00     47539\n",
      "\n",
      "    accuracy                           1.00     55612\n",
      "   macro avg       1.00      1.00      1.00     55612\n",
      "weighted avg       1.00      1.00      1.00     55612\n",
      "\n",
      "precision_score : 1.0\n",
      "accuracy_score : 1.0\n",
      "recall_score : 1.0\n",
      "f1_score : 1.0\n",
      "roc_auc_score : 1.0\n",
      "True_positive : 47539, False_positive : 0, True_negative : 8073, False_negative : 0\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      8073\n",
      "         1.0       1.00      1.00      1.00     47539\n",
      "\n",
      "    accuracy                           1.00     55612\n",
      "   macro avg       1.00      1.00      1.00     55612\n",
      "weighted avg       1.00      1.00      1.00     55612\n",
      "\n",
      "precision_score : 1.0\n",
      "accuracy_score : 1.0\n",
      "recall_score : 1.0\n",
      "f1_score : 1.0\n",
      "roc_auc_score : 1.0\n",
      "True_positive : 47539, False_positive : 0, True_negative : 8073, False_negative : 0\n",
      "ExtraTreesClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      8073\n",
      "         1.0       1.00      1.00      1.00     47539\n",
      "\n",
      "    accuracy                           1.00     55612\n",
      "   macro avg       1.00      1.00      1.00     55612\n",
      "weighted avg       1.00      1.00      1.00     55612\n",
      "\n",
      "precision_score : 1.0\n",
      "accuracy_score : 1.0\n",
      "recall_score : 1.0\n",
      "f1_score : 1.0\n",
      "roc_auc_score : 1.0\n",
      "True_positive : 47539, False_positive : 0, True_negative : 8073, False_negative : 0\n"
     ]
    }
   ],
   "source": [
    "# evaluate base models\n",
    "evaluate_models(X_val, y_val, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 24.252958297729492 seconds ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      8073\n",
      "         1.0       1.00      1.00      1.00     47539\n",
      "\n",
      "    accuracy                           1.00     55612\n",
      "   macro avg       1.00      1.00      1.00     55612\n",
      "weighted avg       1.00      1.00      1.00     55612\n",
      "\n",
      "precision_score : 1.0\n",
      "accuracy_score : 1.0\n",
      "recall_score : 1.0\n",
      "f1_score : 1.0\n",
      "roc_auc_score : 1.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate meta model\n",
    "start_time = time.time() \n",
    "y_pred = super_learner_predictions(X_val, models, meta_model)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(classification_report(y_val,y_pred))\n",
    "print('precision_score : '+str(precision_score(y_val, y_pred, average='weighted')))\n",
    "print('accuracy_score : '+str(accuracy_score(y_val, y_pred)))\n",
    "print('recall_score : '+str(recall_score(y_val, y_pred, average='weighted')))\n",
    "print('f1_score : '+str(f1_score(y_val, y_pred, average='weighted')))\n",
    "print('roc_auc_score : '+str(roc_auc_score(y_val,y_pred))) # TruePositive,TrueNegative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True_positive : 47539, False_positive : 0, True_negative : 8073, False_negative : 0\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "print('True_positive : '+str(tp)+', False_positive : '+str(fp)+', True_negative : '+str(tn)+', False_negative : '+str(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models(models, meta_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
